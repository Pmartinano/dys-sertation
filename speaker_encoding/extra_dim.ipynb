{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ENCODING_MELGANVC",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "V00rptcdKSbq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bcc413cc-e984-4d21-86cb-36705fad2365"
      },
      "source": [
        " #We'll be using TF 2.1 and torchaudio\n",
        "\n",
        "try:\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "import tensorflow as tf\n",
        "!pip install soundfile                    #to save wav files\n",
        "!pip install --no-deps torchaudio==0.5.0\n",
        "!pip install numpy==1.19.5\n"
      ],
      "execution_count": 183,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: soundfile in /usr/local/lib/python3.7/dist-packages (0.10.3.post1)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.7/dist-packages (from soundfile) (1.14.6)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.0->soundfile) (2.20)\n",
            "Requirement already satisfied: torchaudio==0.5.0 in /usr/local/lib/python3.7/dist-packages (0.5.0)\n",
            "Requirement already satisfied: numpy==1.19.5 in /usr/local/lib/python3.7/dist-packages (1.19.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CAmiyxtl2J5s"
      },
      "source": [
        "#Connecting Drive to save model checkpoints during training and to use custom data, uncomment if needed\n",
        "\n",
        "# import os\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LEvqwT96l_Yq"
      },
      "source": [
        "#Imports\n",
        "\n",
        "from __future__ import print_function, division\n",
        "from glob import glob\n",
        "import scipy\n",
        "import soundfile as sf\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import clear_output\n",
        "from tensorflow.keras.layers import Permute, Lambda, Embedding, Input, Dense, Reshape, Flatten, Concatenate, Conv2D, Conv2DTranspose, GlobalAveragePooling2D, UpSampling2D, LeakyReLU, ReLU, Add, Multiply, Lambda, Dot, BatchNormalization, Activation, ZeroPadding2D, Cropping2D, Cropping1D\n",
        "from tensorflow.keras.models import Sequential, Model, load_model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.initializers import TruncatedNormal, he_normal\n",
        "import tensorflow.keras.backend as K\n",
        "import datetime\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import collections\n",
        "from PIL import Image\n",
        "from skimage.transform import resize\n",
        "import imageio\n",
        "import librosa\n",
        "import librosa.display\n",
        "from librosa.feature import melspectrogram\n",
        "import os\n",
        "import time\n",
        "import IPython"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KbaM4WKrvO7r"
      },
      "source": [
        "#Hyperparameters\n",
        "\n",
        "hop=80               #hop size (window size = 6*hop)\n",
        "sr=22050              #sampling rate\n",
        "min_level_db=-100     #reference values to normalize data\n",
        "ref_level_db=20\n",
        "\n",
        "shape=24              #length of time axis of split specrograms to feed to generator            \n",
        "vec_len=128           #length of vector generated by siamese vector\n",
        "bs = 16               #batch size\n",
        "delta = 2.            #constant for siamese loss"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K9pIPj9hnyJ0"
      },
      "source": [
        "#There seems to be a problem with Tensorflow STFT, so we'll be using pytorch to handle offline mel-spectrogram generation and waveform reconstruction\n",
        "#For waveform reconstruction, a gradient-based method is used:\n",
        "\n",
        "\"\"\" ********************NOTE: This is used for mel-spectrogram generation through torchaudio and waveform reconstruction through a gradient based method. It's offline; this means, it's\n",
        "used during training. CONSIDER if it will be necessary to alter the waveform reconstruction if extra input (encoding) is added. \"\"\"\n",
        "\n",
        "''' Decorsière, Rémi, Peter L. Søndergaard, Ewen N. MacDonald, and Torsten Dau. \n",
        "\"Inversion of auditory spectrograms, traditional spectrograms, and other envelope representations.\" \n",
        "IEEE/ACM Transactions on Audio, Speech, and Language Processing 23, no. 1 (2014): 46-56.'''\n",
        "\n",
        "#ORIGINAL CODE FROM https://github.com/yoyololicon/spectrogram-inversion\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "from functools import partial\n",
        "import math\n",
        "import heapq\n",
        "from torchaudio.transforms import MelScale, Spectrogram\n",
        "\n",
        "torch.set_default_tensor_type('torch.cuda.FloatTensor') #Requires GPU\n",
        "\n",
        "specobj = Spectrogram(n_fft=6*hop, win_length=6*hop, hop_length=hop, pad=0, power=2, normalized=True) #Transform that creates spectrogram from audio signal\n",
        "specfunc = specobj.forward \n",
        "melobj = MelScale(n_mels=hop, sample_rate=sr, f_min=0.) #Turns STFT into a mel frequency STFT\n",
        "melfunc = melobj.forward\n",
        "\n",
        "def melspecfunc(waveform): #function that transforms audio file into spectrogram\n",
        "  specgram = specfunc(waveform)\n",
        "  mel_specgram = melfunc(specgram)\n",
        "  return mel_specgram\n",
        "\n",
        "def spectral_convergence(input, target): #Unsure what this is. I assume it's a method for calculating spectral difference\n",
        "    return 20 * ((input - target).norm().log10() - target.norm().log10())\n",
        "\n",
        "def GRAD(spec, transform_fn, samples=None, init_x0=None, maxiter=1000, tol=1e-6, verbose=1, evaiter=10, lr=0.003): #For waveform reconstruction, a gradient-based method is used:\n",
        "\n",
        "    spec = torch.Tensor(spec) #takes in spectrogram\n",
        "    samples = (spec.shape[-1]*hop)-hop #takes last dimension \n",
        "\n",
        "    if init_x0 is None:\n",
        "        init_x0 = spec.new_empty((1,samples)).normal_(std=1e-6)\n",
        "    x = nn.Parameter(init_x0)\n",
        "    T = spec\n",
        "\n",
        "    criterion = nn.L1Loss()\n",
        "    optimizer = torch.optim.Adam([x], lr=lr)\n",
        "\n",
        "    bar_dict = {}\n",
        "    metric_func = spectral_convergence\n",
        "    bar_dict['spectral_convergence'] = 0\n",
        "    metric = 'spectral_convergence'\n",
        "\n",
        "    init_loss = None\n",
        "    with tqdm(total=maxiter, disable=not verbose) as pbar:\n",
        "        for i in range(maxiter):\n",
        "            optimizer.zero_grad()\n",
        "            V = transform_fn(x)\n",
        "            loss = criterion(V, T)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            lr = lr*0.9999\n",
        "            for param_group in optimizer.param_groups:\n",
        "              param_group['lr'] = lr\n",
        "\n",
        "            if i % evaiter == evaiter - 1:\n",
        "                with torch.no_grad():\n",
        "                    V = transform_fn(x)\n",
        "                    bar_dict[metric] = metric_func(V, spec).item()\n",
        "                    l2_loss = criterion(V, spec).item()\n",
        "                    pbar.set_postfix(**bar_dict, loss=l2_loss)\n",
        "                    pbar.update(evaiter)\n",
        "\n",
        "    return x.detach().view(-1).cpu() #detach returns new tensor detached from current graph, with a different shape\n",
        "\n",
        "def normalize(S):\n",
        "  return np.clip((((S - min_level_db) / -min_level_db)*2.)-1., -1, 1) #-1,1 normalization\n",
        "\n",
        "def denormalize(S):\n",
        "  return (((np.clip(S, -1, 1)+1.)/2.) * -min_level_db) + min_level_db #denormalization\n",
        "\n",
        "def prep(wv,hop=80):\n",
        "  S = np.array(torch.squeeze(melspecfunc(torch.Tensor(wv).view(1,-1))).detach().cpu()) #I believe this is for online spectrogram creation. Transforms tensor\n",
        "  S = librosa.power_to_db(S)-ref_level_db #by changing shape, turning it into mel, squeezing it, detaching from graph, operating in cpu and turning into np.\n",
        "  return normalize(S) #it then transforms it to db \n",
        "\n",
        "def deprep(S):\n",
        "  S = denormalize(S)+ref_level_db\n",
        "  S = librosa.db_to_power(S)\n",
        "  wv = GRAD(np.expand_dims(S,0), melspecfunc, maxiter=2000, evaiter=10, tol=1e-8) #reconstruction of waveform through denormalisation, return to power and gradient method\n",
        "  return np.array(np.squeeze(wv))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YNRYjsCDqDjF"
      },
      "source": [
        "#Helper functions\n",
        "\n",
        "#Generate spectrograms from waveform array\n",
        "def tospec(data):\n",
        "  specs=np.empty(data.shape[0], dtype=object) #return array of certain size without initializing entries\n",
        "  for i in range(data.shape[0]):\n",
        "    x = data[i]\n",
        "    S=prep(x)\n",
        "    S = np.array(S, dtype=np.float32)\n",
        "    specs[i]=np.expand_dims(S, -1)\n",
        "  print(specs.shape)\n",
        "  return specs\n",
        "\n",
        "#Generate multiple spectrograms with a determined length from single wav file\n",
        "def tospeclong(path, length=4*22050):\n",
        "  x, sr = librosa.load(path,sr=22050)\n",
        "  x,_ = librosa.effects.trim(x)\n",
        "  loudls = librosa.effects.split(x, top_db=50)\n",
        "  xls = np.array([])\n",
        "  for interv in loudls:\n",
        "    xls = np.concatenate((xls,x[interv[0]:interv[1]]))\n",
        "  x = xls\n",
        "  num = x.shape[0]//length\n",
        "  specs=np.empty(num, dtype=object)\n",
        "  for i in range(num-1):\n",
        "    a = x[i*length:(i+1)*length]\n",
        "    S = prep(a)\n",
        "    S = np.array(S, dtype=np.float32)\n",
        "    try:\n",
        "      sh = S.shape\n",
        "      specs[i]=S\n",
        "    except AttributeError:\n",
        "      print('spectrogram failed')\n",
        "  print(specs.shape)\n",
        "  return specs\n",
        "\n",
        "#Waveform array from path of folder containing wav files\n",
        "def audio_array(path):\n",
        "  ls = glob(f'{path}/*.wav')\n",
        "  adata = []\n",
        "  for i in range(len(ls)):\n",
        "    x, sr = tf.audio.decode_wav(tf.io.read_file(ls[i]), 1)\n",
        "    x = np.array(x, dtype=np.float32)\n",
        "    adata.append(x)\n",
        "  return np.array(adata)\n",
        "\n",
        "#Concatenate spectrograms in array along the time axis\n",
        "def testass(a):\n",
        "  but=False\n",
        "  con = np.array([])\n",
        "  nim = a.shape[0]\n",
        "  for i in range(nim):\n",
        "    im = a[i]\n",
        "    im = np.squeeze(im)\n",
        "    if not but:\n",
        "      con=im\n",
        "      but=True\n",
        "    else:\n",
        "      con = np.concatenate((con,im), axis=1)\n",
        "  return np.squeeze(con)\n",
        "\n",
        "#Split spectrograms in chunks with equal size\n",
        "def splitcut(data):\n",
        "  ls = []\n",
        "  mini = 0\n",
        "  minifinal = 10*shape                                                              #max spectrogram length\n",
        "  for i in range(data.shape[0]-1):\n",
        "    if data[i].shape[1]<=data[i+1].shape[1]:\n",
        "      mini = data[i].shape[1]\n",
        "    else:\n",
        "      mini = data[i+1].shape[1]\n",
        "    if mini>=3*shape and mini<minifinal:\n",
        "      minifinal = mini\n",
        "  for i in range(data.shape[0]):\n",
        "    x = data[i]\n",
        "    if x.shape[1]>=3*shape:\n",
        "      for n in range(x.shape[1]//minifinal):\n",
        "        ls.append(x[:,n*minifinal:n*minifinal+minifinal,:])\n",
        "      ls.append(x[:,-minifinal:,:])\n",
        "  return np.array(ls)\n",
        "\n",
        "\n",
        "  "
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fAAxMBZp-pQZ"
      },
      "source": [
        "speaker_ids = {'F01':1, 'F03':2, 'F04':3, 'M01':4, 'M02':5, 'M03':6, 'M04':7, 'M05': 8}\n",
        "\n",
        "def create_speaker_vector(speaker_dict):\n",
        "  speaker_codes = []\n",
        "  for key, value in speaker_dict.items():\n",
        "    speaker_codes.append(value * 1.11111)\n",
        "  \n",
        "  return speaker_codes\n",
        "\n",
        "speaker_codes = create_speaker_vector(speaker_ids)\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tK_UnhfMELHD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97cabba9-19c9-46a6-bf6a-bf2ec04324c3"
      },
      "source": [
        "#Generating Mel-Spectrogram dataset (Uncomment where needed)\n",
        "#adata: source spectrograms\n",
        "#bdata: target spectrograms\n",
        "\n",
        "\n",
        "##SPEAKER EMBEDDINGS - Maybe we can process all input files separately, add a hash at the very beginning here, and then add them all to the adata array. \n",
        "#Then, during training, the generator will be fed a different array depending on the hash that is present on the spectrogram\n",
        "\n",
        "\n",
        "#CONTROL1\n",
        "\n",
        "awv = audio_array('/content/INP')   \n",
        "aspec = tospec(awv)                                                            \n",
        "adata = splitcut(aspec)      \n",
        "print(adata.shape)\n",
        "\n",
        "\n",
        "\n",
        "#DYSARTHRIC 1\n",
        "m01wv = audio_array('/content/M01')\n",
        "m02wv = audio_array('/content/M02')\n",
        "#m03wv = audio_array()\n",
        "#m04wv = audio_array()\n",
        "#m05wv = audio_array()\n",
        "#f01wv = audio_array()\n",
        "#f03wv = audio_array()\n",
        "#f04wv = audio_array()\n",
        "\n",
        "m01spec = tospec(m01wv)\n",
        "m02spec = tospec(m02wv)\n",
        "#m03spec = tospec(m03wv)\n",
        "#m04spec = tospec(m04wv)\n",
        "#m05spec = tospec(m05wv)\n",
        "#f01spec = tospec(f01wv)\n",
        "#f03spec = tospec(f03wv)\n",
        "#f04spec = tospec(f04wv)\n",
        "\n",
        "\n",
        "m01chunks = splitcut(m01spec)\n",
        "print(f\"m01chunks: {m01chunks.shape}\")\n",
        "#We create another vector of the shape of the chunks\n",
        "batch,mels,timedim,extra = m01chunks.shape #(x, 80, 240, 1)\n",
        "#That vector will be adapted to the target speaker\n",
        "target_vector_m01 = np.ones((batch,1,timedim,extra)) * speaker_ids['M01']\n",
        "print(target_vector_m01.shape)\n",
        "#We concatenate that target vector to the chunks to be able to identify the spectrogram at training.\n",
        "#We will drop this extra dimension after identifying the spectrogram\n",
        "m01chunks = np.concatenate([m01chunks, target_vector_m01], axis=1)\n",
        "print(f\"After concatenation shape of m01chunks: {m01chunks.shape}\")\n",
        "\n",
        "#See if it gives an error and then change hop if necessary\n",
        "\n",
        "#We follow the same steps for all speakers\n",
        "\n",
        "m02chunks = splitcut(m02spec)\n",
        "batch,mels,timedim,extra = m02chunks.shape #(x, 80, 240, 1)\n",
        "target_vector_m02 = np.ones((batch,1,timedim,extra)) * speaker_ids['M02']\n",
        "#print(target_vector_m02[0])\n",
        "m02chunks = np.concatenate([m02chunks, target_vector_m02], axis=1)\n",
        "print(f\"Shape of m02chunks after concat: {m02chunks.shape}\")\n",
        "\n",
        "\n",
        "\n",
        "#m03chunks = splitcut(m03spec)\n",
        "#m04chunks = splitcut(m04spec)\n",
        "#m05chunks = splitcut(m05spec)\n",
        "#f01chunks = splitcut(f01spec)\n",
        "#f03chunks = splitcut(f03spec)\n",
        "#f04chunks = splitcut(f04spec)\n",
        "\n",
        "  \n",
        "#After preprocessing, join all chunks into input file bdata\n",
        "bdata = np.append(m01chunks, m02chunks, axis=0) \n",
        "\n",
        "#bdata = np.append(m01chunks, m02chunks, m03chunks, m04chunks, m05chunks, f01chunks, f03chunks, f04chunks, axis=0) \n",
        "\n",
        "#print(bdata[15][1][0][0]) #Chunk from M01 speaker\n",
        "#print(bdata[20][1][0][0]) #Chunk from M02 speaker\n",
        "print(bdata.shape)\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(3,)\n",
            "(15, 80, 240, 1)\n",
            "(2,)\n",
            "(3,)\n",
            "m01chunks: (19, 80, 240, 1)\n",
            "(19, 1, 240, 1)\n",
            "After concatenation shape of m01chunks: (19, 81, 240, 1)\n",
            "Shape of m02chunks after concat: (24, 81, 240, 1)\n",
            "(43, 81, 240, 1)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:45: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qSesIbwr_GyO"
      },
      "source": [
        "#Creating Tensorflow Datasets\n",
        "\n",
        "@tf.function\n",
        "def procb(x):\n",
        "  return tf.image.random_crop(x, size=[hop+1, 3*shape, 1]) #hop is only changed for b dataset to add the speaker code. It will be cropped again before training.\n",
        "\n",
        "def proca(x):\n",
        "  return tf.image.random_crop(x, size=[hop, 3*shape, 1]) \n",
        "\n",
        "#Problem with code for embeddings is found here - doing dataset_from_tensor_slice, as well as map, makes the code change position to a random place\n",
        "dsa = tf.data.Dataset.from_tensor_slices(adata).repeat(50).map(proca, num_parallel_calls=tf.data.experimental.AUTOTUNE).shuffle(10000).batch(bs, drop_remainder=True)\n",
        "dsb = tf.data.Dataset.from_tensor_slices(bdata).repeat(50).map(procb, num_parallel_calls=tf.data.experimental.AUTOTUNE).shuffle(10000).batch(bs, drop_remainder=True)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "U1BDDpgXLh52",
        "outputId": "9428c70b-d90d-45c8-c936-189f86af5f16"
      },
      "source": [
        "\"\"\"#BATCH DATASET B\n",
        "m01value = [4.]\n",
        "m02value = [5.]\n",
        "\n",
        "true_counter = 0\n",
        "false_counter = 0\n",
        "total_counter = 0\n",
        "\n",
        "for spec in dsb:\n",
        "  #print(spec.shape)\n",
        "  for batch in spec:\n",
        "    total_counter += 1\n",
        "    speaker_value = batch[-1][-1]\n",
        "    if speaker_value == m01value:\n",
        "      #print(True)\n",
        "      true_counter += 1\n",
        "    else:\n",
        "      #print(False)\n",
        "      false_counter += 1\n",
        "    #print(batch[-1])\n",
        "\n",
        "print(true_counter)\n",
        "print(false_counter)\n",
        "print(total_counter)\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'#BATCH DATASET B\\nm01value = [4.]\\nm02value = [5.]\\n\\ntrue_counter = 0\\nfalse_counter = 0\\ntotal_counter = 0\\n\\nfor spec in dsb:\\n  #print(spec.shape)\\n  for batch in spec:\\n    total_counter += 1\\n    speaker_value = batch[-1][-1]\\n    if speaker_value == m01value:\\n      #print(True)\\n      true_counter += 1\\n    else:\\n      #print(False)\\n      false_counter += 1\\n    #print(batch[-1])\\n\\nprint(true_counter)\\nprint(false_counter)\\nprint(total_counter)'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 301
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AHnP2zr7Ypgi"
      },
      "source": [
        "#Adding Spectral Normalization to convolutional layers\n",
        "\n",
        "from tensorflow.python.keras.utils import conv_utils\n",
        "from tensorflow.python.ops import array_ops\n",
        "from tensorflow.python.ops import math_ops\n",
        "from tensorflow.python.ops import sparse_ops\n",
        "from tensorflow.python.ops import gen_math_ops\n",
        "from tensorflow.python.ops import standard_ops\n",
        "from tensorflow.python.eager import context\n",
        "from tensorflow.python.framework import tensor_shape\n",
        "\n",
        "def l2normalize(v, eps=1e-12):\n",
        "    return v / (tf.norm(v) + eps)\n",
        "\n",
        "\n",
        "class ConvSN2D(tf.keras.layers.Conv2D):\n",
        "\n",
        "    def __init__(self, filters, kernel_size, power_iterations=1, **kwargs):\n",
        "        super(ConvSN2D, self).__init__(filters, kernel_size, **kwargs)\n",
        "        self.power_iterations = power_iterations\n",
        "\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        super(ConvSN2D, self).build(input_shape)\n",
        "\n",
        "        if self.data_format == 'channels_first':\n",
        "            channel_axis = 1\n",
        "        else:\n",
        "            channel_axis = -1\n",
        "\n",
        "        self.u = self.add_weight(self.name + '_u',\n",
        "            shape=tuple([1, self.kernel.shape.as_list()[-1]]), \n",
        "            initializer=tf.initializers.RandomNormal(0, 1),\n",
        "            trainable=False\n",
        "        )\n",
        "\n",
        "    def compute_spectral_norm(self, W, new_u, W_shape):\n",
        "        for _ in range(self.power_iterations):\n",
        "\n",
        "            new_v = l2normalize(tf.matmul(new_u, tf.transpose(W)))\n",
        "            new_u = l2normalize(tf.matmul(new_v, W))\n",
        "            \n",
        "        sigma = tf.matmul(tf.matmul(new_v, W), tf.transpose(new_u))\n",
        "        W_bar = W/sigma\n",
        "\n",
        "        with tf.control_dependencies([self.u.assign(new_u)]):\n",
        "          W_bar = tf.reshape(W_bar, W_shape)\n",
        "\n",
        "        return W_bar\n",
        "\n",
        "\n",
        "    def call(self, inputs):\n",
        "        W_shape = self.kernel.shape.as_list()\n",
        "        W_reshaped = tf.reshape(self.kernel, (-1, W_shape[-1]))\n",
        "        new_kernel = self.compute_spectral_norm(W_reshaped, self.u, W_shape)\n",
        "        outputs = self._convolution_op(inputs, new_kernel)\n",
        "\n",
        "        if self.use_bias:\n",
        "            if self.data_format == 'channels_first':\n",
        "                    outputs = tf.nn.bias_add(outputs, self.bias, data_format='NCHW')\n",
        "            else:\n",
        "                outputs = tf.nn.bias_add(outputs, self.bias, data_format='NHWC')\n",
        "        if self.activation is not None:\n",
        "            return self.activation(outputs)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "class ConvSN2DTranspose(tf.keras.layers.Conv2DTranspose):\n",
        "\n",
        "    def __init__(self, filters, kernel_size, power_iterations=1, **kwargs):\n",
        "        super(ConvSN2DTranspose, self).__init__(filters, kernel_size, **kwargs)\n",
        "        self.power_iterations = power_iterations\n",
        "\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        super(ConvSN2DTranspose, self).build(input_shape)\n",
        "\n",
        "        if self.data_format == 'channels_first':\n",
        "            channel_axis = 1\n",
        "        else:\n",
        "            channel_axis = -1\n",
        "\n",
        "        self.u = self.add_weight(self.name + '_u',\n",
        "            shape=tuple([1, self.kernel.shape.as_list()[-1]]), \n",
        "            initializer=tf.initializers.RandomNormal(0, 1),\n",
        "            trainable=False\n",
        "        )\n",
        "\n",
        "    def compute_spectral_norm(self, W, new_u, W_shape):\n",
        "        for _ in range(self.power_iterations):\n",
        "\n",
        "            new_v = l2normalize(tf.matmul(new_u, tf.transpose(W)))\n",
        "            new_u = l2normalize(tf.matmul(new_v, W))\n",
        "            \n",
        "        sigma = tf.matmul(tf.matmul(new_v, W), tf.transpose(new_u))\n",
        "        W_bar = W/sigma\n",
        "\n",
        "        with tf.control_dependencies([self.u.assign(new_u)]):\n",
        "          W_bar = tf.reshape(W_bar, W_shape)\n",
        "\n",
        "        return W_bar\n",
        "\n",
        "    def call(self, inputs):\n",
        "        W_shape = self.kernel.shape.as_list()\n",
        "        W_reshaped = tf.reshape(self.kernel, (-1, W_shape[-1]))\n",
        "        new_kernel = self.compute_spectral_norm(W_reshaped, self.u, W_shape)\n",
        "\n",
        "        inputs_shape = array_ops.shape(inputs)\n",
        "        batch_size = inputs_shape[0]\n",
        "        if self.data_format == 'channels_first':\n",
        "          h_axis, w_axis = 2, 3\n",
        "        else:\n",
        "          h_axis, w_axis = 1, 2\n",
        "\n",
        "        height, width = inputs_shape[h_axis], inputs_shape[w_axis]\n",
        "        kernel_h, kernel_w = self.kernel_size\n",
        "        stride_h, stride_w = self.strides\n",
        "\n",
        "        if self.output_padding is None:\n",
        "          out_pad_h = out_pad_w = None\n",
        "        else:\n",
        "          out_pad_h, out_pad_w = self.output_padding\n",
        "\n",
        "        out_height = conv_utils.deconv_output_length(height,\n",
        "                                                    kernel_h,\n",
        "                                                    padding=self.padding,\n",
        "                                                    output_padding=out_pad_h,\n",
        "                                                    stride=stride_h,\n",
        "                                                    dilation=self.dilation_rate[0])\n",
        "        out_width = conv_utils.deconv_output_length(width,\n",
        "                                                    kernel_w,\n",
        "                                                    padding=self.padding,\n",
        "                                                    output_padding=out_pad_w,\n",
        "                                                    stride=stride_w,\n",
        "                                                    dilation=self.dilation_rate[1])\n",
        "        if self.data_format == 'channels_first':\n",
        "          output_shape = (batch_size, self.filters, out_height, out_width)\n",
        "        else:\n",
        "          output_shape = (batch_size, out_height, out_width, self.filters)\n",
        "\n",
        "        output_shape_tensor = array_ops.stack(output_shape)\n",
        "        outputs = K.conv2d_transpose(\n",
        "            inputs,\n",
        "            new_kernel,\n",
        "            output_shape_tensor,\n",
        "            strides=self.strides,\n",
        "            padding=self.padding,\n",
        "            data_format=self.data_format,\n",
        "            dilation_rate=self.dilation_rate)\n",
        "\n",
        "        if not context.executing_eagerly():\n",
        "          out_shape = self.compute_output_shape(inputs.shape)\n",
        "          outputs.set_shape(out_shape)\n",
        "\n",
        "        if self.use_bias:\n",
        "          outputs = tf.nn.bias_add(\n",
        "              outputs,\n",
        "              self.bias,\n",
        "              data_format=conv_utils.convert_data_format(self.data_format, ndim=4))\n",
        "\n",
        "        if self.activation is not None:\n",
        "          return self.activation(outputs)\n",
        "        return outputs  \n",
        "\n",
        "class DenseSN(Dense):\n",
        "    def build(self, input_shape):\n",
        "        super(DenseSN, self).build(input_shape)\n",
        "\n",
        "        self.u = self.add_weight(self.name + '_u',\n",
        "            shape=tuple([1, self.kernel.shape.as_list()[-1]]), \n",
        "            initializer=tf.initializers.RandomNormal(0, 1),\n",
        "            trainable=False)\n",
        "        \n",
        "    def compute_spectral_norm(self, W, new_u, W_shape):\n",
        "        new_v = l2normalize(tf.matmul(new_u, tf.transpose(W)))\n",
        "        new_u = l2normalize(tf.matmul(new_v, W))\n",
        "        sigma = tf.matmul(tf.matmul(new_v, W), tf.transpose(new_u))\n",
        "        W_bar = W/sigma\n",
        "        with tf.control_dependencies([self.u.assign(new_u)]):\n",
        "          W_bar = tf.reshape(W_bar, W_shape)\n",
        "        return W_bar\n",
        "        \n",
        "    def call(self, inputs):\n",
        "        W_shape = self.kernel.shape.as_list()\n",
        "        W_reshaped = tf.reshape(self.kernel, (-1, W_shape[-1]))\n",
        "        new_kernel = self.compute_spectral_norm(W_reshaped, self.u, W_shape)\n",
        "        rank = len(inputs.shape)\n",
        "        if rank > 2:\n",
        "          outputs = standard_ops.tensordot(inputs, new_kernel, [[rank - 1], [0]])\n",
        "          if not context.executing_eagerly():\n",
        "            shape = inputs.shape.as_list()\n",
        "            output_shape = shape[:-1] + [self.units]\n",
        "            outputs.set_shape(output_shape)\n",
        "        else:\n",
        "          inputs = math_ops.cast(inputs, self._compute_dtype)\n",
        "          if K.is_sparse(inputs):\n",
        "            outputs = sparse_ops.sparse_tensor_dense_matmul(inputs, new_kernel)\n",
        "          else:\n",
        "            outputs = gen_math_ops.mat_mul(inputs, new_kernel)\n",
        "        if self.use_bias:\n",
        "          outputs = tf.nn.bias_add(outputs, self.bias)\n",
        "        if self.activation is not None:\n",
        "          return self.activation(outputs)\n",
        "        return outputs\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eX41awYeHE1N"
      },
      "source": [
        "#Networks Architecture\n",
        "\n",
        "init = tf.keras.initializers.he_uniform()\n",
        "\n",
        "def conv2d(layer_input, filters, kernel_size=4, strides=2, padding='same', leaky=True, bnorm=True, sn=True):\n",
        "  if leaky:\n",
        "    Activ = LeakyReLU(alpha=0.2)\n",
        "  else:\n",
        "    Activ = ReLU()\n",
        "  if sn:\n",
        "    d = ConvSN2D(filters, kernel_size=kernel_size, strides=strides, padding=padding, kernel_initializer=init, use_bias=False)(layer_input)\n",
        "  else:\n",
        "    d = Conv2D(filters, kernel_size=kernel_size, strides=strides, padding=padding, kernel_initializer=init, use_bias=False)(layer_input)\n",
        "  if bnorm:\n",
        "    d = BatchNormalization()(d)\n",
        "  d = Activ(d)\n",
        "  return d\n",
        "\n",
        "def deconv2d(layer_input, layer_res, filters, kernel_size=4, conc=True, scalev=False, bnorm=True, up=True, padding='same', strides=2):\n",
        "  if up:\n",
        "    u = UpSampling2D((1,2))(layer_input)\n",
        "    u = ConvSN2D(filters, kernel_size, strides=(1,1), kernel_initializer=init, use_bias=False, padding=padding)(u)\n",
        "  else:\n",
        "    u = ConvSN2DTranspose(filters, kernel_size, strides=strides, kernel_initializer=init, use_bias=False, padding=padding)(layer_input)\n",
        "  if bnorm:\n",
        "    u = BatchNormalization()(u)\n",
        "  u = LeakyReLU(alpha=0.2)(u)\n",
        "  if conc:\n",
        "    u = Concatenate()([u,layer_res])\n",
        "  return u\n",
        "\n",
        "#Extract function: splitting spectrograms\n",
        "def extract_image(im):\n",
        "  im1 = Cropping2D(((0,0), (0, 2*(im.shape[2]//3))))(im)\n",
        "  im2 = Cropping2D(((0,0), (im.shape[2]//3,im.shape[2]//3)))(im)\n",
        "  im3 = Cropping2D(((0,0), (2*(im.shape[2]//3), 0)))(im)\n",
        "  return im1,im2,im3\n",
        "\n",
        "#Assemble function: concatenating spectrograms\n",
        "def assemble_image(lsim):\n",
        "  im1,im2,im3 = lsim\n",
        "  imh = Concatenate(2)([im1,im2,im3])\n",
        "  return imh\n",
        "\n",
        "#U-NET style architecture\n",
        "def build_generator(input_shapes):\n",
        "  h,w,c = input_shapes # 80, 24, 1, except at test time, where batch size usually > 16\n",
        "  # Inputs\n",
        "  inp = Input(shape=input_shapes) # shape: 80, 24, 1\n",
        "  speaker_v_input = Input(shape=(w))\n",
        "  print(f\"Shape of speaker input: {speaker_v_input.shape}\") #None, 24\n",
        "  # Getting embeddings\n",
        "  embedding_out = tf.keras.layers.Embedding(8, 2, input_length=w)(speaker_v_input) # assume 8 possible speakers, 2 is the dimensionality of embedding.> None, 24, 2\n",
        "  print(f\"Embedding out shape: {embedding_out.shape}\")\n",
        "  #Change shape to (None, 2, 24)\n",
        "  embedding_out = Permute([2,1])(embedding_out) #None, 2, 24\n",
        "  embedding_out = tf.expand_dims(embedding_out, axis=-1) #None, 2, 24, 1\n",
        "  concat_out = Concatenate(axis=1)([inp, embedding_out]) # Concatenate embedding to input > None, 82, 24, 1\n",
        "    \n",
        "  # Processing concatenated input\n",
        "  g0 = tf.keras.layers.ZeroPadding2D((0,1))(concat_out)\n",
        "  g1 = conv2d(g0, 256, kernel_size=(h+2,3), strides=1, padding='valid')\n",
        "  g2 = conv2d(g1, 256, kernel_size=(1,9), strides=(1,2))\n",
        "  g3 = conv2d(g2, 256, kernel_size=(1,7), strides=(1,2))\n",
        "  #upscaling\n",
        "  g4 = deconv2d(g3,g2, 256, kernel_size=(1,7), strides=(1,2))\n",
        "  g5 = deconv2d(g4,g1, 256, kernel_size=(1,9), strides=(1,2), bnorm=False)\n",
        "  g6 = ConvSN2DTranspose(1, kernel_size=(h,1), strides=(1,1), kernel_initializer=init, padding='valid', activation='tanh')(g5)\n",
        "  print(f\"Shape of g6: {g6.shape}\")\n",
        "  \n",
        "  return Model([inp, speaker_v_input],g6, name='G')\n",
        "\n",
        "\n",
        "#Siamese Network\n",
        "def build_siamese(input_shape):\n",
        "  h,w,c = input_shape\n",
        "  inp = Input(shape=(input_shape))\n",
        "\n",
        "  g1 = conv2d(inp, 256, kernel_size=(h,3), strides=1, padding='valid', sn=False)\n",
        "  g2 = conv2d(g1, 256, kernel_size=(1,9), strides=(1,2), sn=False)\n",
        "  g3 = conv2d(g2, 256, kernel_size=(1,7), strides=(1,2), sn=False)\n",
        "  g4 = Flatten()(g3)\n",
        "  g5 = Dense(vec_len)(g4)\n",
        "  return Model(inp, g5, name='S')\n",
        "\n",
        "#Discriminator (Critic) Network\n",
        "def build_critic(input_shape):\n",
        "  h,w,c = input_shape\n",
        "  inp = Input(shape=(h,w,c)) # shape: 80, 24\n",
        "  print(f\"Input shape of critic: {inp.shape}\")\n",
        "\n",
        "  g1 = conv2d(inp, 512, kernel_size=(h,3), strides=1, padding='valid', bnorm=False)\n",
        "  g2 = conv2d(g1, 512, kernel_size=(1,9), strides=(1,2), bnorm=False)\n",
        "  g3 = conv2d(g2, 512, kernel_size=(1,7), strides=(1,2), bnorm=False)\n",
        "  g4 = Flatten()(g3)\n",
        "  print(f\"Critic after flattening: {g4}\")\n",
        "  g4 = DenseSN(1, kernel_initializer=init)(g4)\n",
        "  print(g4)\n",
        "  return Model(inp, g4, name='C')"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4fXJmItOzrhC"
      },
      "source": [
        "#Load past models from path to resume training or test\n",
        "def load(path):\n",
        "  gen = build_generator((hop,shape,1))\n",
        "  siam = build_siamese((hop,shape,1))\n",
        "  critic = build_critic((hop,3*shape,1))\n",
        "  gen.load_weights(path+'/gen.h5')\n",
        "  critic.load_weights(path+'/critic.h5')\n",
        "  siam.load_weights(path+'/siam.h5')\n",
        "  return gen,critic,siam\n",
        "\n",
        "#Build models\n",
        "def build():\n",
        "  gen = build_generator((hop,shape,1))\n",
        "  siam = build_siamese((hop,shape,1))\n",
        "  critic = build_critic((hop,3*shape,1))                                          #the discriminator accepts as input spectrograms of triple the width of those generated by the generator\n",
        "  return gen,critic,siam\n",
        "\n",
        "#Generate a random batch to display current training results\n",
        "def testgena():\n",
        "  sw = True\n",
        "  while sw:\n",
        "    a = np.random.choice(aspec)\n",
        "    if a.shape[1]//shape!=1:\n",
        "      sw=False\n",
        "  dsa = []\n",
        "  if a.shape[1]//shape>6:\n",
        "    num=6\n",
        "  else:\n",
        "    num=a.shape[1]//shape\n",
        "  rn = np.random.randint(a.shape[1]-(num*shape))\n",
        "  for i in range(num):\n",
        "    im = a[:,rn+(i*shape):rn+(i*shape)+shape]\n",
        "    im = np.reshape(im, (im.shape[0],im.shape[1],1))\n",
        "    dsa.append(im)\n",
        "  return np.array(dsa, dtype=np.float32)\n",
        "\n",
        "#Show results mid-training\n",
        "def save_test_image_full(path):\n",
        "  #Change this so it accommodates to different speakers\n",
        "  a = testgena()\n",
        "  print(a.shape)\n",
        "  bs = a.shape[0]\n",
        "  generating_vec = np.full(shape=(bs,24), fill_value=2) #We create a different speaker vector at test time, since the batch size wil be != 16\n",
        "  ab = gen([a,generating_vec], training=False) #Change if we want to vary the speakers\n",
        "  print(\"Results mid-training for generation of random spectrograms and audio files. \")\n",
        "  #gen.summary()\n",
        "  ab = testass(ab)\n",
        "  a = testass(a)\n",
        "  abwv = deprep(ab)\n",
        "  awv = deprep(a)\n",
        "  sf.write(path+'/new_file.wav', abwv, sr)\n",
        "  IPython.display.display(IPython.display.Audio(np.squeeze(abwv), rate=sr))\n",
        "  IPython.display.display(IPython.display.Audio(np.squeeze(awv), rate=sr))\n",
        "  fig, axs = plt.subplots(ncols=2)\n",
        "  axs[0].imshow(np.flip(a, -2), cmap=None)\n",
        "  axs[0].axis('off')\n",
        "  axs[0].set_title('Source')\n",
        "  axs[1].imshow(np.flip(ab, -2), cmap=None)\n",
        "  axs[1].axis('off')\n",
        "  axs[1].set_title('Generated')\n",
        "  plt.show()\n",
        "\n",
        "#Save in training loop\n",
        "def save_end(epoch,gloss,closs,mloss,n_save=3,save_path='../content/'):                 #use custom save_path (i.e. Drive '../content/drive/My Drive/')\n",
        "  if epoch % n_save == 0:\n",
        "    print('Saving...')\n",
        "    path = f'{save_path}/MELGANVC-{str(gloss)[:9]}-{str(closs)[:9]}-{str(mloss)[:9]}'\n",
        "    os.mkdir(path)\n",
        "    gen.save_weights(path+'/gen.h5')\n",
        "    critic.save_weights(path+'/critic.h5')\n",
        "    siam.save_weights(path+'/siam.h5')\n",
        "    #save_test_image_full(path)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fn2s65AxjDJ8"
      },
      "source": [
        "#Losses\n",
        "\n",
        "def mae(x,y):\n",
        "  return tf.reduce_mean(tf.abs(x-y))\n",
        "\n",
        "def mse(x,y):\n",
        "  return tf.reduce_mean((x-y)**2)\n",
        "\n",
        "def loss_travel(sa,sab,sa1,sab1):\n",
        "  l1 = tf.reduce_mean(((sa-sa1) - (sab-sab1))**2)\n",
        "  l2 = tf.reduce_mean(tf.reduce_sum(-(tf.nn.l2_normalize(sa-sa1, axis=[-1]) * tf.nn.l2_normalize(sab-sab1, axis=[-1])), axis=-1))\n",
        "  return l1+l2\n",
        "\n",
        "def loss_siamese(sa,sa1):\n",
        "  logits = tf.sqrt(tf.reduce_sum((sa-sa1)**2, axis=-1, keepdims=True))\n",
        "  return tf.reduce_mean(tf.square(tf.maximum((delta - logits), 0)))\n",
        "\n",
        "def d_loss_f(fake):\n",
        "  return tf.reduce_mean(tf.maximum(1 + fake, 0))\n",
        "\n",
        "def d_loss_r(real):\n",
        "  return tf.reduce_mean(tf.maximum(1 - real, 0))\n",
        "\n",
        "def g_loss_f(fake):\n",
        "  return tf.reduce_mean(- fake)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fgjxHjyIhPwl"
      },
      "source": [
        "#Get models and optimizers\n",
        "def get_networks(shape, load_model=False, path=None):\n",
        "  if not load_model:\n",
        "    gen,critic,siam = build()\n",
        "  else:\n",
        "    gen,critic,siam = load(path)\n",
        "  print('Built networks')\n",
        "\n",
        "  opt_gen = Adam(0.0001, 0.5)\n",
        "  opt_disc = Adam(0.0001, 0.5)\n",
        "\n",
        "  return gen,critic,siam, [opt_gen,opt_disc]\n",
        "\n",
        "#Set learning rate\n",
        "def update_lr(lr):\n",
        "  opt_gen.learning_rate = lr\n",
        "  opt_disc.learning_rate = lr"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8n7cIPUQC5Y"
      },
      "source": [
        "\n",
        "import sys\n",
        "def check_target(target_spec):\n",
        "\n",
        "  print(\"Inside check_target\")\n",
        "  target_vec = np.random.rand(16,24)\n",
        "  m01value = [4.]\n",
        "  m02value = [5.]\n",
        "  imaginaryvalue = [12123.]\n",
        "  loop_n = -1\n",
        "\n",
        "  print(f\"target_spec.shape: {target_spec.shape}\")\n",
        "\n",
        "  for example in target_spec:\n",
        "    loop_n += 1\n",
        "    for code in example[-1][-1]:\n",
        "      if tf.math.equal(code, m01value) == True:\n",
        "        target_vec[loop_n] = np.ones((1, shape)) * 4 \n",
        "        tf.print(code, output_stream=sys.stdout)\n",
        "        print(f\"Speaker is M01\")\n",
        "        print(target_vec[loop_n])\n",
        "      elif tf.math.equal(code, m02value) == True:\n",
        "        target_vec[loop_n] = np.ones((1, shape)) * 5\n",
        "        tf.print(code, output_stream=sys.stdout)\n",
        "        print(f\"Speaker is M02\")\n",
        "        print(target_vec[loop_n]) \n",
        "      elif tf.math.equal(code, imaginaryvalue) == True:\n",
        "        print(\"NOT WORKING\")\n",
        "\n",
        "  return target_vec[loop_n]\n",
        "\n",
        "\n"
      ],
      "execution_count": 180,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ZZ9vmUDvzml",
        "outputId": "4bde0ad5-4ab8-4cfe-ea98-7a092d72ba8e"
      },
      "source": [
        "#Check_target works outside of training.\n",
        "#Why is it not working during training, and it returns all values at the same time? \n",
        "#That seems to be the problem, as the loss goes up and down. It seems like the target_vector is being assigned randomly\n",
        "#What if each batch has multiple speakers??\n",
        "\n",
        "counter = 0\n",
        "for batchi,(a,b) in enumerate(zip(dsa,dsb)): #Using this because it's what is being used during training (16,81,24,1)\n",
        "  target_v = check_target(b)\n",
        "  print(target_v)\n",
        "  break\n",
        "\n"
      ],
      "execution_count": 181,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Inside check_target\n",
            "target_spec.shape: (16, 81, 72, 1)\n",
            "5\n",
            "Speaker is M02\n",
            "[5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5.]\n",
            "4\n",
            "Speaker is M01\n",
            "[4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4.]\n",
            "5\n",
            "Speaker is M02\n",
            "[5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5.]\n",
            "4\n",
            "Speaker is M01\n",
            "[4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4.]\n",
            "4\n",
            "Speaker is M01\n",
            "[4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4.]\n",
            "4\n",
            "Speaker is M01\n",
            "[4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4.]\n",
            "5\n",
            "Speaker is M02\n",
            "[5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5.]\n",
            "5\n",
            "Speaker is M02\n",
            "[5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5.]\n",
            "4\n",
            "Speaker is M01\n",
            "[4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4.]\n",
            "4\n",
            "Speaker is M01\n",
            "[4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4.]\n",
            "4\n",
            "Speaker is M01\n",
            "[4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4.]\n",
            "5\n",
            "Speaker is M02\n",
            "[5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5.]\n",
            "5\n",
            "Speaker is M02\n",
            "[5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5.]\n",
            "4\n",
            "Speaker is M01\n",
            "[4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4.]\n",
            "5\n",
            "Speaker is M02\n",
            "[5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5.]\n",
            "5\n",
            "Speaker is M02\n",
            "[5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5.]\n",
            "[5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WGWjgHqDWR78"
      },
      "source": [
        "#Training Functions\n",
        "\n",
        "#Train Generator, Siamese and Critic\n",
        "\n",
        "@tf.function\n",
        "def train_all(a,b):\n",
        "  #splitting spectrogram in 3 parts\n",
        "  target_speaker = check_target(b)\n",
        "  print(f\"B SHAPE: {b.shape}\")\n",
        "  print(f\"A SHAPE: {a.shape}\")\n",
        "  bcropped = tf.image.random_crop(b, size=[bs, 80, 72, 1]) #crop 81st dimension that has the speaker identity values\n",
        "  aa,aa2,aa3 = extract_image(a) \n",
        "  print(f\"B SHAPE AFTER CROPPING: {bcropped.shape}\")\n",
        "  bb,bb2,bb3 = extract_image(bcropped)\n",
        "  #print(f\"Target values: {bb[1][1][0]}\")\n",
        "\n",
        "  with tf.GradientTape() as tape_gen, tf.GradientTape() as tape_disc:\n",
        "\n",
        "    print(f\"target_speaker shape: {target_speaker.shape}\")\n",
        "    fab = gen([aa, target_speaker], training=True)\n",
        "    fab2 = gen([aa2, target_speaker], training=True)\n",
        "    fab3 = gen([aa3, target_speaker], training=True)\n",
        "    #identity mapping B to B                                                        COMMENT THESE 3 LINES IF THE IDENTITY LOSS TERM IS NOT NEEDED\n",
        "    fid = gen([bb, target_speaker], training=True) \n",
        "    fid2 = gen([bb2, target_speaker], training=True)\n",
        "    fid3 = gen([bb3, target_speaker], training=True)\n",
        "    #concatenate/assemble converted spectrograms\n",
        "    fabtot = assemble_image([fab,fab2,fab3])\n",
        "    #print(f\"Fabtot shape: {fabtot.shape}\")\n",
        "    #print(\"GENERATOR SUMMARY\")\n",
        "    #gen.summary()\n",
        "\n",
        "    #feed concatenated spectrograms to critic\n",
        "    cab = critic(fabtot, training=True)\n",
        "    cb = critic(bcropped, training=True)\n",
        "    #print(\"CRITIC SUMMARY \")\n",
        "    #critic.summary()D\n",
        "\n",
        "    #feed 2 pairs (A,G(A)) extracted spectrograms to Siamese\n",
        "    \n",
        "    sab = siam(fab, training=True)\n",
        "    sab2 = siam(fab3, training=True)\n",
        "    sa = siam(aa, training=True)\n",
        "    sa2 = siam(aa3, training=True)\n",
        "    #print(\"SIAMESE SUMMARY\")\n",
        "    #siam.summary()\n",
        "\n",
        "    #identity mapping loss\n",
        "    loss_id = (mae(bb,fid)+mae(bb2,fid2)+mae(bb3,fid3))/3.                         #loss_id = 0. IF THE IDENTITY LOSS TERM IS NOT NEEDED\n",
        "    #travel loss\n",
        "    loss_m = loss_travel(sa,sab,sa2,sab2)+loss_siamese(sa,sa2)\n",
        "    #generator and critic losses\n",
        "    loss_g = g_loss_f(cab)\n",
        "    loss_dr = d_loss_r(cb)\n",
        "    loss_df = d_loss_f(cab)\n",
        "    loss_d = (loss_dr+loss_df)/2.\n",
        "    #generator+siamese total loss\n",
        "    lossgtot = loss_g+10.*loss_m+0.5*loss_id                                       #CHANGE LOSS WEIGHTS HERE  (COMMENT OUT +w*loss_id IF THE IDENTITY LOSS TERM IS NOT NEEDED)\n",
        "  \n",
        "  #computing and applying gradients\n",
        "  grad_gen = tape_gen.gradient(lossgtot, gen.trainable_variables+siam.trainable_variables)\n",
        "  opt_gen.apply_gradients(zip(grad_gen, gen.trainable_variables+siam.trainable_variables))\n",
        "\n",
        "  grad_disc = tape_disc.gradient(loss_d, critic.trainable_variables)\n",
        "  opt_disc.apply_gradients(zip(grad_disc, critic.trainable_variables))\n",
        "  \n",
        "  return loss_dr,loss_df,loss_g,loss_id\n",
        "\n",
        "#Train critic only\n",
        "@tf.function\n",
        "def train_d(a,b):\n",
        "  aa,aa2,aa3 = extract_image(a)\n",
        "  target_speaker = check_target(b)\n",
        "  bcropped = tf.image.random_crop(b, size=[bs, 80, 72, 1])\n",
        "\n",
        "  with tf.GradientTape() as tape_disc:\n",
        "\n",
        "    fab = gen([aa,target_speaker], training=True)\n",
        "    fab2 = gen([aa2,target_speaker], training=True)\n",
        "    fab3 = gen([aa3,target_speaker], training=True)\n",
        "    fabtot = assemble_image([fab,fab2,fab3])\n",
        "\n",
        "    cab = critic(fabtot, training=True)\n",
        "    cb = critic(bcropped, training=True)\n",
        "\n",
        "    loss_dr = d_loss_r(cb)\n",
        "    loss_df = d_loss_f(cab)\n",
        "\n",
        "    loss_d = (loss_dr+loss_df)/2.\n",
        "  \n",
        "  grad_disc = tape_disc.gradient(loss_d, critic.trainable_variables)\n",
        "  opt_disc.apply_gradients(zip(grad_disc, critic.trainable_variables))\n",
        "\n",
        "  return loss_dr,loss_df"
      ],
      "execution_count": 176,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aVwL-Ry-nNru"
      },
      "source": [
        "\n",
        "#Training Loop\n",
        "\n",
        "def train(epochs, batch_size=16, lr=0.0001, n_save=6, gupt=5):\n",
        "  \n",
        "  update_lr(lr)\n",
        "  df_list = []\n",
        "  dr_list = []\n",
        "  g_list = []\n",
        "  id_list = []\n",
        "  c = 0\n",
        "  g = 0\n",
        "  \n",
        "  for epoch in range(epochs):\n",
        "        #bef = time.time()\n",
        "        \n",
        "        for batchi,(a,b) in enumerate(zip(dsa,dsb)):\n",
        "          \n",
        "            if batchi%gupt==0:\n",
        "              dloss_t,dloss_f,gloss,idloss = train_all(a,b)\n",
        "            else:\n",
        "              dloss_t,dloss_f = train_d(a,b)\n",
        "\n",
        "            df_list.append(dloss_f)\n",
        "            dr_list.append(dloss_t)\n",
        "            g_list.append(gloss)\n",
        "            id_list.append(idloss)\n",
        "            c += 1\n",
        "            g += 1\n",
        "\n",
        "            if batchi%600==0:\n",
        "                print(f'[Epoch {epoch}/{epochs}] [Batch {batchi}] [D loss f: {np.mean(df_list[-g:], axis=0)} ', end='')\n",
        "                print(f'r: {np.mean(dr_list[-g:], axis=0)}] ', end='')\n",
        "                print(f'[G loss: {np.mean(g_list[-g:], axis=0)}] ', end='')\n",
        "                print(f'[ID loss: {np.mean(id_list[-g:])}] ', end='')\n",
        "                print(f'[LR: {lr}]')\n",
        "                g = 0\n",
        "            nbatch=batchi\n",
        "\n",
        "        #print(f'Time/Batch {(time.time()-bef)/nbatch}')\n",
        "        save_end(epoch,np.mean(g_list[-n_save*c:], axis=0),np.mean(df_list[-n_save*c:], axis=0),np.mean(id_list[-n_save*c:], axis=0),n_save=n_save)\n",
        "        print(f'Mean D loss: {np.mean(df_list[-c:], axis=0)} Mean G loss: {np.mean(g_list[-c:], axis=0)} Mean ID loss: {np.mean(id_list[-c:], axis=0)}')\n",
        "        c = 0"
      ],
      "execution_count": 177,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JruweKNrl_ZD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "082e9f06-c29b-4dd6-9d33-7653c871536f"
      },
      "source": [
        "#Build models and initialize optimizers\n",
        "\n",
        "#If load_model=True, specify the path where the models are saved\n",
        "\n",
        "gen,critic,siam, [opt_gen,opt_disc] = get_networks(shape, load_model=False, path='../content/drive/My Drive/')"
      ],
      "execution_count": 178,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of speaker input: (None, 24)\n",
            "Embedding out shape: (None, 24, 2)\n",
            "Shape of g6: (None, 80, 24, 1)\n",
            "Input shape of critic: (None, 80, 72, 1)\n",
            "Critic after flattening: KerasTensor(type_spec=TensorSpec(shape=(None, 9216), dtype=tf.float32, name=None), name='flatten_13/Reshape:0', description=\"created by layer 'flatten_13'\")\n",
            "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='dense_sn_6/BiasAdd:0', description=\"created by layer 'dense_sn_6'\")\n",
            "Built networks\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BknKCA-8yqap",
        "cellView": "both",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 901
        },
        "outputId": "c648bc6a-a971-43da-878b-846649afdc64"
      },
      "source": [
        "#Training\n",
        "\n",
        "#n_save = how many epochs between each saving and displaying of results\n",
        "#gupt = how many discriminator updates for generator+siamese update\n",
        "\n",
        "train(10, batch_size=bs, lr=0.0002, n_save=1, gupt=5)"
      ],
      "execution_count": 179,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Inside check_target\n",
            "target_spec.shape: (16, 81, 72, 1)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NotImplementedError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-179-e04c7dd971fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#gupt = how many discriminator updates for generator+siamese update\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0002\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_save\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgupt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-177-f29ef72e2d3c>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epochs, batch_size, lr, n_save, gupt)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbatchi\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mgupt\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m               \u001b[0mdloss_t\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdloss_f\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgloss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0midloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m               \u001b[0mdloss_t\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdloss_f\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    931\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 933\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    934\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    758\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m    759\u001b[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0;32m--> 760\u001b[0;31m             *args, **kwds))\n\u001b[0m\u001b[1;32m    761\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    762\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minvalid_creator_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0munused_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0munused_kwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3064\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3065\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3066\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3067\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3068\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3462\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3463\u001b[0;31m           \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3464\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3465\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3306\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3307\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3308\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   3309\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3310\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes, acd_record_initial_resource_uses)\u001b[0m\n\u001b[1;32m   1005\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1007\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1008\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1009\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    666\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    667\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompile_with_xla\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 668\u001b[0;31m           \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    669\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    992\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    993\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 994\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    995\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotImplementedError\u001b[0m: in user code:\n\n    <ipython-input-176-4aede5c4268c>:8 train_all  *\n        target_speaker = check_target(b)\n    <ipython-input-174-808dcf7e9ab4>:14 check_target  *\n        for example in target_spec:\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/operators/control_flow.py:418 for_stmt\n        iter_, extra_test, body, get_state, set_state, symbol_names, opts)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/operators/control_flow.py:523 _known_len_tf_for_stmt\n        opts,\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/operators/control_flow.py:1111 _tf_while_stmt\n        aug_test, aug_body, aug_init_vars, **while_loop_opts)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/control_flow_ops.py:2738 while_loop\n        back_prop=back_prop)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/while_v2.py:167 while_loop\n        add_control_dependencies=add_control_dependencies)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py:1007 func_graph_from_py_func\n        func_outputs = python_func(*func_args, **func_kwargs)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/while_v2.py:146 wrapped_cond\n        pred = cond(*_pack_sequence_as(orig_loop_vars, args))\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/operators/control_flow.py:1076 aug_test\n        set_state(loop_vars)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/operators/control_flow.py:502 aug_set_state\n        set_state(loop_vars)\n    /tmp/tmp5by53inf.py:24 set_state_4\n        (loop_n, target_vec[loop_n]) = vars_\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py:870 __array__\n        \" a NumPy call, which is not supported\".format(self.name))\n\n    NotImplementedError: Cannot convert a symbolic Tensor (while/Placeholder_1:0) to a numpy array. This error may indicate that you're trying to pass a Tensor to a NumPy call, which is not supported\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-f6nSiF95H-"
      },
      "source": [
        "#After Training, use these functions to convert data with the generator and save the results\n",
        "\n",
        "#Assembling generated Spectrogram chunks into final Spectrogram\n",
        "def specass(a,spec):\n",
        "  but=False\n",
        "  con = np.array([])\n",
        "  nim = a.shape[0]\n",
        "  for i in range(nim-1):\n",
        "    im = a[i]\n",
        "    im = np.squeeze(im)\n",
        "    if not but:\n",
        "      con=im\n",
        "      but=True\n",
        "    else:\n",
        "      con = np.concatenate((con,im), axis=1)\n",
        "  diff = spec.shape[1]-(nim*shape)\n",
        "  a = np.squeeze(a)\n",
        "  con = np.concatenate((con,a[-1,:,-diff:]), axis=1)\n",
        "  return np.squeeze(con)\n",
        "\n",
        "#Splitting input spectrogram into different chunks to feed to the generator\n",
        "def chopspec(spec):\n",
        "  dsa=[]\n",
        "  for i in range(spec.shape[1]//shape):\n",
        "    im = spec[:,i*shape:i*shape+shape]\n",
        "    im = np.reshape(im, (im.shape[0],im.shape[1],1))\n",
        "    dsa.append(im)\n",
        "  imlast = spec[:,-shape:]\n",
        "  imlast = np.reshape(imlast, (imlast.shape[0],imlast.shape[1],1))\n",
        "  print(imlast.shape)\n",
        "  dsa.append(imlast)\n",
        "  print(np.array(dsa, dtype=np.float32).shape)\n",
        "\n",
        "  return np.array(dsa, dtype=np.float32)\n",
        "\n",
        "#Converting from source Spectrogram to target Spectrogram\n",
        "def towave(spec, name, path='../content/', show=False):\n",
        "  specarr = chopspec(spec) #Chop source spectrogram into chunks of equal size\n",
        "  print(f\"specarr: {specarr.shape}\")\n",
        "  a = specarr \n",
        "  batch_size = specarr.shape[0]\n",
        "  testing_vector = np.ones((batch_size, shape)) * 4\n",
        "  #print(testing_vector)\n",
        "  ab = gen([a, testing_vector], training=False) \n",
        "  #gen.summary()\n",
        " \n",
        "  print('Assembling and Converting...')\n",
        "  a = specass(a,spec)\n",
        "  ab = specass(ab,spec)\n",
        "  awv = deprep(a)\n",
        "  abwv = deprep(ab)\n",
        "  print('Saving...')\n",
        "  pathfin = f'{path}/{name}'\n",
        "  os.mkdir(pathfin)\n",
        "  sf.write(pathfin+'/AB.wav', abwv, sr)\n",
        "  sf.write(pathfin+'/A.wav', awv, sr)\n",
        "  print('Saved WAV!')\n",
        "  IPython.display.display(IPython.display.Audio(np.squeeze(abwv), rate=sr))\n",
        "  IPython.display.display(IPython.display.Audio(np.squeeze(awv), rate=sr))\n",
        "  if show:\n",
        "    fig, axs = plt.subplots(ncols=2)\n",
        "    axs[0].imshow(np.flip(a, -2), cmap=None)\n",
        "    axs[0].axis('off')\n",
        "    axs[0].set_title('Source')\n",
        "    axs[1].imshow(np.flip(ab, -2), cmap=None)\n",
        "    axs[1].axis('off')\n",
        "    axs[1].set_title('Generated')\n",
        "    plt.show()\n",
        "  return abwv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6FZE91V1BIJX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 553
        },
        "outputId": "54b7e58c-85cc-431e-ec7f-bbea5c4ae1e4"
      },
      "source": [
        "#Wav to wav conversion\n",
        "\n",
        "wv, sr = librosa.load('/content/test/MC01_30005.wav', sr=22050)  #Load waveform\n",
        "print(wv.shape)\n",
        "speca = prep(wv)                                                    #Waveform to Spectrogram\n",
        "\n",
        "plt.figure(figsize=(50,1))                                          #Show Spectrogram\n",
        "plt.imshow(np.flip(speca, axis=0), cmap=None)\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "abwv = towave(speca, name='FILENAME12', path='../content/')           #Convert and save wav"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(56448,)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 3600x72 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "(80, 24, 1)\n",
            "(30, 80, 24, 1)\n",
            "specarr: (30, 80, 24, 1)\n",
            "Assembling and Converting...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 2000/2000 [00:02<00:00, 680.24it/s, loss=0.000162, spectral_convergence=-19.6]\n",
            " 62%|██████▏   | 1240/2000 [00:01<00:01, 674.85it/s, loss=0.00868, spectral_convergence=-15.5]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-318-0febd00a7611>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mabwv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtowave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspeca\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'FILENAME12'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'../content/'\u001b[0m\u001b[0;34m)\u001b[0m           \u001b[0;31m#Convert and save wav\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-317-a74d3d8c64b3>\u001b[0m in \u001b[0;36mtowave\u001b[0;34m(spec, name, path, show)\u001b[0m\n\u001b[1;32m     49\u001b[0m   \u001b[0mab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspecass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mab\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m   \u001b[0mawv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeprep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m   \u001b[0mabwv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeprep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Saving...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m   \u001b[0mpathfin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'{path}/{name}'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-294-5bf3a0f849da>\u001b[0m in \u001b[0;36mdeprep\u001b[0;34m(S)\u001b[0m\n\u001b[1;32m     89\u001b[0m   \u001b[0mS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdenormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mref_level_db\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m   \u001b[0mS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlibrosa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdb_to_power\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m   \u001b[0mwv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGRAD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmelspecfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-8\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#reconstruction of waveform through denormalisation, return to power and gradient method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-294-5bf3a0f849da>\u001b[0m in \u001b[0;36mGRAD\u001b[0;34m(spec, transform_fn, samples, init_x0, maxiter, tol, verbose, evaiter, lr)\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m             \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m0.9999\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mparam_group\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    116\u001b[0m                    \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m                    \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'weight_decay'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m                    eps=group['eps'])\n\u001b[0m\u001b[1;32m    119\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/_functional.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate_steps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0mbias_correction1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m         \u001b[0mbias_correction2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}